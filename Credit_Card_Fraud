Importing dependencies

[1]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
[20]
1s
# loading the dataset to a Pandas DataFrame
credit_card_data = pd.read_csv('/content/creditcard.csv')
[21]
0s
#first 5 rows of the dataset
credit_card_data.head()

[22]
0s
credit_card_data.tail()


[23]
0s
# dataset information
credit_card_data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 59511 entries, 0 to 59510
Data columns (total 31 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   Time    59511 non-null  int64  
 1   V1      59511 non-null  float64
 2   V2      59511 non-null  float64
 3   V3      59511 non-null  float64
 4   V4      59511 non-null  float64
 5   V5      59511 non-null  float64
 6   V6      59511 non-null  float64
 7   V7      59511 non-null  float64
 8   V8      59511 non-null  float64
 9   V9      59511 non-null  float64
 10  V10     59511 non-null  float64
 11  V11     59511 non-null  float64
 12  V12     59511 non-null  float64
 13  V13     59511 non-null  float64
 14  V14     59511 non-null  float64
 15  V15     59511 non-null  float64
 16  V16     59510 non-null  float64
 17  V17     59510 non-null  float64
 18  V18     59510 non-null  float64
 19  V19     59510 non-null  float64
 20  V20     59510 non-null  float64
 21  V21     59510 non-null  float64
 22  V22     59510 non-null  float64
 23  V23     59510 non-null  float64
 24  V24     59510 non-null  float64
 25  V25     59510 non-null  float64
 26  V26     59510 non-null  float64
 27  V27     59510 non-null  float64
 28  V28     59510 non-null  float64
 29  Amount  59510 non-null  float64
 30  Class   59510 non-null  float64
dtypes: float64(30), int64(1)
memory usage: 14.1 MB
[24]
0s
# checking the no. of missing values in each col
credit_card_data.isnull().sum()
Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       1
V17       1
V18       1
V19       1
V20       1
V21       1
V22       1
V23       1
V24       1
V25       1
V26       1
V27       1
V28       1
Amount    1
Class     1
dtype: int64
[25]
0s
# distribution of legit and fraudulent transaction
credit_card_data['Class'].value_counts()
Class
0.0    59348
1.0      162
Name: count, dtype: int64
This dataset is highly unbalanced

[ ]

Start coding or generate with AI.
0 --> Normal transaction

1 --> fradulent transaction

[27]
0s
#separating data for analysis
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]
[28]
0s
print(legit.shape)
print(fraud.shape)
(59348, 31)
(162, 31)
[29]
0s
# statistical measures of the data
legit.Amount.describe()
count    59348.000000
mean        95.423013
std        270.229826
min          0.000000
25%          7.690000
50%         25.985000
75%         87.440000
max      19656.530000
Name: Amount, dtype: float64
[30]
1s
fraud.Amount.describe()
count     162.000000
mean       93.565988
std       224.658775
min         0.000000
25%         1.000000
50%         4.245000
75%        99.990000
max      1809.680000
Name: Amount, dtype: float64
[31]
0s
# compare the values for both transactions
credit_card_data.groupby('Class').mean()

[ ]

Start coding or generate with AI.
Undersampling

[ ]

Start coding or generate with AI.
Build a sample dataset containing similar distribution of normal transaction and fradulent transcation

[ ]

Start coding or generate with AI.
Number of fradulent transaction -->162

[32]
0s
legit_sample = legit.sample(n=162)
[ ]

Start coding or generate with AI.
Concatinating two dataframes

[33]
0s
new_dataset = pd.concat([legit_sample,fraud],axis=0)
[34]
0s
new_dataset.head()

[35]
0s
new_dataset.tail()

[36]
0s
new_dataset['Class'].value_counts()
Class
0.0    162
1.0    162
Name: count, dtype: int64
[37]
0s
new_dataset.groupby('Class').mean()

[ ]

Start coding or generate with AI.
Splitting the data into Features&Targets

[38]
0s
X = new_dataset.drop(columns='Class',axis=1)
Y=new_dataset['Class']
[39]
0s
print(X)
        Time        V1        V2        V3        V4        V5        V6  \
46930  42992  1.390834 -1.320231  0.514912 -1.475554 -1.696711 -0.426602   
22649  32360  1.421744 -1.163190  0.689779 -1.494606 -1.573379 -0.148367   
14222  25273  1.160454 -0.153025  0.753322  0.414598 -0.565553  0.220740   
13025  22878  1.162162 -0.378332  1.129462 -0.476915 -1.202072 -0.512768   
22805  32440  1.471064 -0.871730  0.585444 -1.314480 -1.406031 -0.715330   
...      ...       ...       ...       ...       ...       ...       ...   
57248  47826 -0.887287  1.390002  1.219686  1.661425  1.009228 -0.733908   
57470  47923  0.364377  1.443523 -2.220907  2.036985 -1.237055 -1.728161   
57615  47982 -1.232804  2.244119 -1.703826  1.492536 -1.192985 -1.686110   
58422  48380 -2.790771 -1.464269  1.031165  1.921356 -0.090014 -0.483871   
58761  48533  1.243848  0.524526 -0.538884  1.209196  0.479538 -0.197429   

             V7        V8        V9  ...       V20       V21       V22  \
46930 -1.242454  0.078296 -1.721206  ... -0.317038 -0.330256 -0.824780   
22649 -1.344892  0.179937 -1.818938  ... -0.428321 -0.305645 -0.582111   
14222 -0.629205  0.326686  0.366304  ... -0.192955 -0.046055 -0.201486   
13025 -0.785481  0.054477  3.221198  ... -0.249393 -0.118022  0.119825   
22805 -0.911246 -0.197571 -2.051507  ... -0.311774 -0.156888  0.052965   
...         ...       ...       ...  ...       ...       ...       ...   
57248  0.855829  0.000077 -1.275631  ... -0.268347 -0.083734 -0.346930   
57470 -2.058582  0.358895 -1.393306  ...  0.310980  0.402730 -0.132129   
57615 -1.864612  0.856122 -1.973535  ...  0.207889  0.560475  0.165682   
58422  0.780731 -0.348776  0.609133  ... -1.376721 -0.392667  0.440020   
58761  0.049166  0.037792  0.128119  ... -0.171541 -0.051660 -0.084089   

            V23       V24       V25       V26       V27       V28  Amount  
46930  0.106198 -0.087230  0.069684 -0.406704  0.018646  0.020842   60.00  
22649  0.213850 -0.061085 -0.010860 -0.393565  0.048354  0.013124   10.00  
14222  0.087366 -0.329850  0.050815  0.250837 -0.004009  0.006437    5.99  
13025  0.047595  0.338024  0.325017 -0.713689  0.075638  0.028647   11.85  
22805  0.098254  0.422050  0.314858 -0.200357  0.056254  0.022042    5.00  
...         ...       ...       ...       ...       ...       ...     ...  
57248 -0.050619  0.231044 -0.450760 -0.376205  0.034504  0.157775    7.58  
57470 -0.032977  0.460861  0.560404  0.409366  0.539668  0.296918    0.76  
57615 -0.013754  0.474935 -0.218725  0.302809  0.466031  0.250134    0.76  
58422  0.777659  0.418552  0.244563 -0.159361  0.060540  0.356958  208.58  
58761 -0.192846 -0.917392  0.681953 -0.194419  0.045917  0.040136    1.00  

[324 rows x 30 columns]
[40]
0s
print(Y)
46930    0.0
22649    0.0
14222    0.0
13025    0.0
22805    0.0
        ... 
57248    1.0
57470    1.0
57615    1.0
58422    1.0
58761    1.0
Name: Class, Length: 324, dtype: float64
[ ]

Start coding or generate with AI.
Split the data into Training data and testing data

[41]
0s
X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)
[42]
0s
print(X.shape,X_train.shape,X_test.shape)
(324, 30) (259, 30) (65, 30)
Model training

[ ]

Start coding or generate with AI.
Logistic Regression

[43]
0s
model = LogisticRegression()
[44]
0s
# training the LOgistic Regression Model with Training data
model.fit(X_train,Y_train)

Model Evaluation

[ ]

Start coding or generate with AI.
Accuracy Score

[45]
0s
# accuracy on training data
X_train_prediction=model.predict(X_train)
training_data_accuracy=accuracy_score(X_train_prediction,Y_train)
[46]
0s
print('Accuracy on Training data:', training_data_accuracy)
Accuracy on Training data: 0.9575289575289575
[47]
0s
# accuracy on test data
X_test_prediction=model.predict(X_test)
test_data_accuracy=accuracy_score(X_test_prediction,Y_test)
[48]
0s
print('Accuracy on Testing data:', test_data_accuracy)
Accuracy on Testing data: 0.9076923076923077
